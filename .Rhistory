library(dplyr)
library(knitr)
library(stringr)
library(rsunlight)
zip.code <- '98105'
base.uri <- "https://congress.api.sunlightfoundation.com"
```
District `r congressional.district` is the congressional district that the University of Washington is in, under zip code `r zip.code`.
```{r echo=FALSE}
district.content <- content(GET(paste0(base.uri, paste0("/districts/locate?zip=" , zip.code))), "text")
districts <- fromJSON(district.content)
congressional.district <- districts$results$district
```
Here is a table of your local representatives and some basic information on them. You can also reach them through those links.
```{r echo=FALSE}
legislator.content <- content(GET(paste0(base.uri, paste0("/legislators/locate?zip=" , zip.code))), "text")
legislators <- fromJSON(legislator.content)$results %>% flatten()
# Gets the links for the table
legislators$website <- paste0("[Link](", legislators$website, ")")
legislators$twitter_id <- paste0("[Link](", legislators$twitter_id, ")")
# Only selects necessary information for the table
legislator.info <- select(legislators, "First Name" = first_name, "Last Name" = last_name, "Title" = title, "Party" = party, "Chamber" = chamber, "Phone" = phone, "Website" = website, "Twitter" = twitter_id)
# Creates table in proper format
kable(head(legislator.info, format = "markdown"))
```
Here is a histogram of each legislator and the number of committees they served on.
```{r echo=FALSE}
committees.content <- content(GET(paste0(base.uri, "/committees?member_ids=L000551")), "text")
# Gathers the names of the committees each representative was on
RepresentativeCommittees <- function(rep.id) {
rep.committee.content <- content(GET(paste0(base.uri, paste0("/committees?member_ids=" , rep.id))), "text")
rep.committees <- data.frame(fromJSON(rep.committee.content))
rep.committees$bioguide_id <- rep.id
return(rep.committees)
}
#returns a list of frames of data on rep committees
rep.committees.served <- bind_rows(lapply(legislators$bioguide_id, RepresentativeCommittees))
rep.committees.served <- left_join(rep.committees.served, legislators, by = "bioguide_id")
rep.committees.table <- sort(table(c(rep.committees.served$first_name)))
barplot(rep.committees.table, main="Committees Served on by Each Rep.", horiz=TRUE, xlab = "# of Committees", names.arg=c(paste(legislators$first_name, legislators$last_name)), las = 1, cex.names = .5)
```
The selected committee is `r selected.committee$results.name`. This committee's focus is on the specific problems of American Indian, Native Hawaiian, and Alaska Native people such as, but not limited to, economic, health, education, and land management. Hopefully, the committee through these studies and discussion can propose legislation to alleviate the difficulties faced by these minority groups. Although it entered as a select committee in the 1940s, further necessity has pushed this to become a permanent committee as of 1973. The chair of this committee is `r selected.committee.chairman`. There are `r total.people.on.committee` people on this committee. My representative is on the `r my.rep.side` of this committee. The gender breakdown of this committee is `r percent.female.in.committee`% male and `r percent.female.in.committee'% female.
```{r echo=FALSE}
# Select a committee that is not a subcommittee
not.subcommittee <- filter(rep.committees.served, results.subcommittee == FALSE)
selected.committee <- sample_n(not.subcommittee, 1)
my.rep.id <- selected.committee$bioguide_id
# Get the information on the members of that committee
selected.committee.content <- content(GET(paste0(base.uri, paste0("/committees?committee_id=", selected.committee$results.committee_id, "&fields=members"))), "text")
selected.committee.data <- fromJSON(selected.committee.data)$results$members[[1]] %>% flatten()
# Answer questions about the committee and store them as variables
selected.committee.chairman <- filter(selected.committee.data, title == 'Chairman') %>%
select(legislator.first_name, legislator.last_name)
total.people.on.committee <- nrow(selected.committee.data)
my.rep.side <- filter(selected.committee.data, legislator.bioguide_id == my.rep.id) %>%
select(side)
females.in.committee <- nrow(filter(selected.committee.data, legislator.gender == 'F'))
percent.male.in.committee <- round(((total.people.on.committee - females.in.committee)/total.people.on.committee)*100, digits = 0)
percent.female.in.committee <- round((females.in.committee/total.people.on.committee)*100, digits = 0)
```
selected.committee <- not.subcommittee[8,]
my.rep.id <- selected.committee$bioguide_id
install.packages("ggplot2")
rm(list=ls())
library(ggplot2)
library(dplyr)
library(scales)
BuildBarGraph <- function(data, x.axis, y.axis, x.axis.label, y.axis.label) {
use.data <- select(data, x.axis, y.axis)
colnames(use.data) <- c(x.axis.label, y.axis.label)
graph.title <- paste("Number of", x.axis.label, "in INFO201, Spring 2017")
bar.graph <- ggplot(data = use.data) +
geom_bar(mapping = aes(x = use.data$x.axis))
}
bar.graph <- ggplot(data = use.data) +
geom_bar(mapping = aes(x = use.data$x.axis)) +
xlab(x.axis.label) +
ylab(y.axis.label) +
ggtitle(graph.title)
}
BuildBarGraph <- function(data, x.axis, y.axis, x.axis.label, y.axis.label) {
use.data <- select(data, x.axis, y.axis)
colnames(use.data) <- c(x.axis.label, y.axis.label)
graph.title <- paste("Number of", x.axis.label, "in INFO201, Spring 2017")
bar.graph <- ggplot(data = use.data) +
geom_bar(mapping = aes(x = use.data$x.axis)) +
xlab(x.axis.label) +
ylab(y.axis.label) +
ggtitle(graph.title)
}
dfAustenBooks <- austen_books()
df.austen <- austen_books()
library(janeaustenr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
df.austen <- austen_books()
library(janeaustenr)
install.packages('janeaustenr')
install.packages('tidytext')
library(janeaustenr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
df.austen <- austen_books()
View(df.austen)
no.books <- length(unique(df.austen$book))
# Exercise-1
# Developed from: http://tidytextmining.com/
# Set up (install packages that you don't have)
install.packages('janeaustenr')
install.packages('tidytext')
library(janeaustenr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
# Load booksinto a dataframe using the austen_books() function
df.austen <- austen_books()
# How many books are in the dataset?
no.books <- length(unique(df.austen$book))
# Which book has the most lines?
most.lines <- df.austen %>%
group_by(book) %>%
summarize(lines = n())
# Use the unnest_tokens function to generate the full list of words
# Which words are most common (regardless of which book them come from)?
# Remove stop words by performing an anti_join with the stop_words dataframe
# Which non stop-words are most common?
# Use ggplot to make a horizontal bar chart of the word frequencies of non-stop words
install.packages("tidytext")
install.packages("janeaustenr")
install.packages("janeaustenr")
rm(ls=list())
rm(list = ls())
df.austen <- austen_books()
no.books <- length(unique(df.austen$book))
most.lines <- df.austen %>%
group_by(book) %>%
summarize(lines = n())
unnest_tokens(df.austen)
unnest_tokens(df.austen$text)
unnest_tokens(text)
install.packages('rvest')
install.packages('rvest')
library(rvest)
info.page <- read_html('https://www.washington.edu/students/crscat/info.html')
rm(list=ls())
setwd('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201')
source('./scripts/setup.R')
source('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather/scripts/setup.R')
source('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather/scripts')
twitter.data <- twitterDataByTime
twitter.data <- twitterDataByTime()
shiny::runApp('twitter-weather')
source('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather/scripts/setup.R')
install.packages(streamR)
install.packages('streamR')
library(streamR)
source('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather/scripts/setup.R')
library(shiny)
library(dplyr)
library(plotly)
library(streamR)
library(httr)
library(jsonlite)
library(rgeos)
library(rgdal)
install.packages('rgeos')
install.packages('rgdal')
install.packages('rgdal')
source('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather/scripts')
source('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather/scripts/BuildRenderedChart.R')
source('./scripts/BuildRenderedChart.R')
setwd('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather')
source('scripts/BuildRenderedChart.R')
source('./scripts/BuildRenderedChart.R')
source('scripts/BuildRenderedChart.R')
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
BuildBarPlot <- function(data, x.var, y.var, x.label, y.label, title, color.var) {
p <- plot_ly(data = data,
x = data[[x.var]],
y = data[[y.var]],
type = "bar",
color = color.var #fixxxx
) %>%
layout(
title = title,
xaxis = list(title = x.label),
yaxis = list(title = y.label),
barmode = "group"
)
p <- hide_colorbar(p)
# layout(
#   yaxis = list(range = c(0, 100))
# )
return(p)
}
setwd('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather')
source('scripts/BuildRenderedChart.R')
source('/scripts/BuildRenderedChart.R')
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
source("BuildBarChart.R")
# The plot1 variable determines the y axis, therefore, choose the plot that
# has a higher y max
RenderPlots <- function(plot.1, data.1, y.var.1, plot.2, data.2, y.var.2) {
plot.3 <- plot_ly(data = data,
x = data.1[[y.var.1]],
y = data.2[[y.var.2]],
type = "scatter",
marker = list(size = 20,
color = data[[color.var]],
line = list(color = 'rgba(0, 0, 0, .8)',
width = 2),
opacity = 0.7))
return(subplot(plot.1, plot.2, plot.3, shareX = TRUE))
}
source('.scripts/BuildRenderedChart.R')
View(mtcars)
output$mainPlot <- BuildBarPlot(mtcars, 'hp', 'drat', "MPG", "CYL", "mpg v cyl")
runApp()
runApp()
runApp()
install.packages('anytime')
source('scripts/BuildRenderedChart.R')
source('scripts/BuildBarChart.R')
runApp()
BuildBarPlot(mtcars, 'hp', 'drat', "MPG", "CYL", "mpg v cyl")
library(shiny)
install.packages('anytime')
library(anytime)
runApp()
runApp()
source('./scripts')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
BuildBarPlot <- function(data, x.var, y.var, x.label, y.label, title, color.var) {
p <- plot_ly(data = data,
x = data[[x.var]],
y = data[[y.var]],
type = "bar",
color = color.var #fixxxx
) %>%
layout(
title = title,
xaxis = list(title = x.label),
yaxis = list(title = y.label),
barmode = "group"
)
p <- hide_colorbar(p)
# layout(
#   yaxis = list(range = c(0, 100))
# )
return(p)
}
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
BuildLinePlot <- function(data, x.var, y.var, x.label, y.label, title, color.var){
p <- plot_ly(data = data,
x = data[[x.var]],
y = data[[y.var]],
type = "scatter",
marker = list(size = 20,
color = data[[color.var]],
line = list(color = 'rgba(0, 0, 0, .8)',
width = 2),
opacity = 0.7)) %>%
layout(title = title,
yaxis = list(title = x.label),
xaxis = list(title = y.label))
return(p)
}
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
# The plot1 variable determines the y axis, therefore, choose the plot that
# has a higher y max
RenderPlots <- function(plot.1, data.1, y.var.1, plot.2, data.2, y.var.2) {
plot.3 <- plot_ly(data = data,
x = data.1[[y.var.1]],
y = data.2[[y.var.2]],
type = "scatter",
marker = list(size = 20,
color = data[[color.var]],
line = list(color = 'rgba(0, 0, 0, .8)',
width = 2),
opacity = 0.7))
return(subplot(plot.1, plot.2, plot.3, shareX = TRUE))
}
library(rtweet)
library(jsonlite)
library(rgeos)
library(rgdal)
library(httr)
library(dplyr)
library(anytime)
# Latitude & Longitude Retrieval for API Calls
# --------------------------------------------
# Code for findLatLong and findGeoData sourced from:
# https://stackoverflow.com/posts/27868207/revisions
# Returns a data frame that contains the longitude and latitude
# for the given state and city.
# Input format: findLatLong(geog_db, "Portland", "ME")
# Ex: lon       lat       city      state
#     -70.25404 43.66186  Portland   ME
findLatLong <- function(geo_db, city, state) {
do.call(rbind.data.frame, mapply(function(x, y) {
geo_db %>% filter(city==x, state==y)
}, city, state, SIMPLIFY=FALSE))
}
findGeoData <- function() {
try({
GET("http://www.mapcruzin.com/fcc-wireless-shapefiles/cities-towns.zip",
write_disk("cities.zip"))
unzip("cities.zip", exdir="cities") })
# reads in shape file from URL
shp <- readOGR("cities/citiesx020.shp", "citiesx020")
# extract city centroids from shape file with name and state
geo.data <-
gCentroid(shp, byid = TRUE) %>%
data.frame() %>%
rename(lon = x, lat = y) %>%
mutate(city = shp@data$NAME, state = shp@data$STATE)
return(geo.data)
}
# Global Variables
# ----------------
# Options list for states and capital cities (Mara)
cities <- c("Montgomery, Alabama", "Juneau, Alaska", "Phoenix, Arizona",
"Little Rock, Arkansas", "Sacramento, California", "Denver, Colorado",
"Hartford, Connecticut", "Dover, Delaware", "Tallahassee, Florida",
"Atlanta, Georgia", "Honolulu, Hawaii", "Boise, Idaho", "Springfield, Illinois",
"Indianapolis, Indiana", "Des Moines, Iowa", "Topeka, Kansas", "Frankfort, Kentucky",
"Baton Rouge, Louisiana", "Augusta, Maine", "Annapolis, Maryland", "Boston, Massachusetts",
"Lansing, Michigan", "St. Paul, Minnesota", "Jackson, Mississippi", "Jefferson City, Missouri",
"Helena, Montana", "Lincoln, Nebraska", "Carson City, Nevada", "Concord, New Hampshire",
"Trenton, New Jersey", "Santa Fe, New Mexico", "Albany, New York", "Raleigh, North Carolina",
"Bismarck, North Dakota", "Columbus, Ohio", "Oklahoma City, Oklahoma", "Salem, Oregon",
"Harrisburg, Pennsylvania", "Providence, Rhode Island", "Columbia, South Carolina",
"Pierre, South Dakota", "Nashville, Tennessee", "Austin, Texas", "Salt Lake City, Utah",
"Montpelier, Vermont", "Richmond, Virginia", "Olympia, Washington", "Charleston, West Virginia",
"Madison, Wisconsin", "Cheyenne, Wyoming"
)
# API Calls - Data Retrieval
# -------------------------
# TO-DO: Twitter API - adjust twitterData to get tweets from specific day
# What range should we do for our search in miles?
# How many tweets should we retrieve to ensure that we're getting all of them?
## See retryonratelimit for search_tweets in rtweet documentation for more info on getting more than 18000 tweets
# Are there any other libraries that can help retrieve tweets from a specific date range or specific time? No historical data available...
# Retrieves a data frame with the number of tweets for a given state and city.
# Currently retrieves up to 18000 tweets from the last 6-9 days within 5 miles of the
# specified location's latitude and longitude.
twitterData <- function(city, state, day) {
# Retrieves latitude and longitude for the given state and city for API query
lat.long.df <- geo_data %>% findLatLong(city, state)
longitude <- lat.long.df[,1]
latitude <- lat.long.df[,2]
# Gets tweets from specified location and radius.
# Change "5mi" if you want a different area of query; change n to get different number of tweets
twitter.df <- search_tweets(q = " ", geocode=paste0(latitude, ",", longitude, ",","10mi"), n = 1000) ## This takes like 5 minutes...
twitter.df.times <- twitter.df %>% select(created_at)
hourly.range <- cut(twitter.df$created_at, breaks="hour")
twitter.result <- data.frame(table(hourly.range))
return (twitter.result)
}
# test variables
city <- "Portland"
state <- "ME"
day <- "2017-05-28"
# Retrieves a data frame with weather data for the specified day with the given city and state,
# with hourly time block starting from midnight of the day requested,
# continuing until midnight of the following day. Hourly time blocks start from the current system time.
# input format: weatherData("Portland", "ME", "28 May 2017"), multiple Date formats should work
weatherData <- function(city, state, day) {
# Retrieve latitude and longitude for given city and state
lat.long.df <- geo_data %>% findLatLong(city, state)
longitude <- lat.long.df[,1]
latitude <- lat.long.df[,2]
# Convert given Date to UNIX format
unix.time.day <- as.numeric(as.POSIXct(anydate(day)))
# Retrieve API key from key.JSON (stored in JSON for security)
key <- fromJSON(file="access-keys.json")$weather$key
# setting params for API  call
base.url <- "https://api.darksky.net/forecast/"
weather.uri <- paste0(base.url, key, "/", longitude, ",", latitude, ",", unix.time.day)
weather.params <- list(exclude = paste0("currently", ",", "minutely", ",", "flags"))
# retrieving data from API
weather.response <- GET(weather.uri, query = weather.params)
weather.body <- content(weather.response, "text")
weather.results <- fromJSON(weather.body)
weather.df <- weather.results$hourly$data
# convert UNIX time to Dates
weather.df$time <- anytime(weather.df$time)
# convert Celsius temperatures to Fahrenheit
weather.df$temperature <- weather.df$temperature * (9/5) + 32
weather.df <- weather.df %>% select(temperature, time)
return(weather.df)
}
# server.R
#libraries
library(anytime)
library(shiny)
library(dplyr)
library(plotly)
library(httr)
library(rgeos)
library(jsonlite)
library(rgdal)
#scripts
#setwd('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather')
source('scripts/BuildBarChart.R', chdir = T)
# Retrieves dataset for towns and cities in Canada/US with latitudinal and longitudinal data for API calls
geo_data <- read.csv("geo_data.csv")
## Twitter authentification credentials
appname <- "twitter-weather-moscow-mules"
# Retrieving authentication credentials from .json
twitter.key <- fromJSON(txt='access-keys.json')$twitter$consumer_key
twitter.secret <- fromJSON(txt='access-keys.json')$twitter$consumer_secret
# create token for authentication
twitter.token <- create_token(
app = appname,
consumer_key = twitter.key,
consumer_secret = twitter.secret)
# call buildtimeline.R
shinyServer(function(input, output) {
#output$mainPlot <- BuildBarPlot(twitter.data, input$time, y.var, 'Time', 'Number of Tweets', 'Number of Tweets Throughout the Day')
#output$mainPlot <- BuildLinePlot(data, x.var, y.var, x.label, y.label, title)
#output$mainPlot <- RenderPlots(plot.1, y.var.1, plot.2, y.var.2)
output$fooPlot1 <- renderPlotly({
return(BuildBarPlot(mtcars, 'hp', 'drat', "MPG", "CYL", "mpg v cyl"))
})
output$value <- renderPrint({input$dates})
output$value <- renderPrint({input$time})
output$value <- renderPrint({input$city})
output$value <- renderPrint({input$chart})
})
# Read in libraries
library(shiny)
library(plotly)
# Read in source scripts
source('./scripts')
# Create Shiny UI
shinyUI(fluidPage(
# Application title
titlePanel("Twitter and Weather"),
# Sidebar with select inputs for date, time, and city
sidebarLayout(
sidebarPanel(
# Returns YYYY-MM-DD
dateInput("dates", "Select Date"),
# Returns date values as 0-24
sliderInput("time", "Select Time Range",
min = 0, max = 24, value = c(0, 24)),
# Returns Capital City, State
selectInput("city", "Select City", choices = cities),
# Returns Tweets or Weather
selectInput("chart", "Select Graphs", choices = c("Tweets", "Weather"))
),
# Plot it!
mainPanel(
plotlyOutput('fooPlot1', height = "600px", width = "800px")
)
)
)
)
runApp()
