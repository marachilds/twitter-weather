weather.body <- content(weather.response, "text")
weather.results <- fromJSON(weather.body)
weather.df <- weather.results$hourly$data
# convert UNIX time to Dates
weather.df$time <- anytime(weather.df$time)
# convert Celsius temperatures to Fahrenheit
weather.df$temperature <- weather.df$temperature * (9/5) + 32
weather.df <- weather.df %>% select(temperature, time)
return(weather.df)
}
View(twitter.df.times)
location <- "Seattle, Washington"
seper <- paste(shQuote(location), collapse=", ")
View(seper)
seper <- paste(shQuote(location), sep=", ")
View(seper)
library(stringr)
seper <- str_split_fixed(location, ", ")
seper <- str_split_fixed(location, ", ", 2)
View(seper)
location <- str_split_fixed(input$city, ", ", 2)
weather.data <- weatherData(location$V1, location$V2, input$dates)
weatherData(Olympia, Washington, 2017-05-28)
setwd('~/Documents/College/Sophomore (2016-2017)/Spring Quarter/INFO201/twitter-weather')
geo_data <- read.csv("geo_data.csv")
weatherData(Olympia, Washington, 2017-05-28)
weatherData('Olympia', 'Washington', '2017-05-28')
runApp()
runApp()
runApp()
weatherData("Montgomery", "Alabama", "2017-05-28")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
location <- str_split_fixed(input$city, ", ", 2)
View(weather.data)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?create_token
twitter.key <- fromJSON(txt = 'access-keys.json')$twitter$consumer_key
twitter.secret <- fromJSON(txt = 'access-keys.json')$twitter$consumer_secret
twitter.token <- create_token(
app = appname,
consumer_key = twitter.key,
consumer_secret = twitter.secret)
appname <- "twitter-weather-moscow-mules"
twitter.token <- create_token(
app = appname,
consumer_key = twitter.key,
consumer_secret = twitter.secret)
shinyServer(function(input, output) {
#Tweets = bar chart
# output$fooPlot1 <- renderPlotly({
#   location <- str_split_fixed(input$city, ", ", 2)
#   twitter.data <- twitterData(location$V1, location$V2, input$dates)
#
#    return(BuildBarPlot(twitter.data, twitter.data$time, twitter.data$freq, "Time", "Tweets", paste("Number of Tweets on", input$dates, "in", input$city)))
# })
#Weather = line chart
output$fooPlot1 <- renderPlotly({
location <- str_split_fixed(input$city, ", ", 2)
weather.data <- weatherData(location[,1], location[,2], input$dates)
return(BuildLineChart(weather.data, weather.data[,time], weather.data[,temperature], "Time", "Weather", paste("Weather on", input$dates, "in", input$city)))
})
#both = both
# output$fooPlot1 <- renderPlotly({
#   return(BuildRenderedChart(plot.1, data.1, y.var.1, plot.2, data.2, y.var.2))
# })
output$value <- renderPrint({input$dates})
output$value <- renderPrint({input$time})
output$value <- renderPrint({input$city})
output$value <- renderPrint({input$chart})
})
runApp()
?fromJSON
key <- fromJSON(txt = "access-keys.json")$weather$key
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# Read in libraries
library(shiny)
library(plotly)
# Read in source scripts
source('./scripts/setup.R')
# Create Shiny UI
shinyUI(fluidPage(
# Application title
titlePanel("Twitter and Weather"),
# Sidebar with select inputs for date, time, and city
sidebarLayout(
sidebarPanel(
# Returns YYYY-MM-DD
dateInput("start.date", "Select Start Date", min = min.start, max = max.start),
# Returns YYYY-MM-DD
dateInput("end.date", "Select End Date", min = min.end, max = max.end),
# Returns Capital City, State
selectInput("city", "Select City", choices = cities),
# Returns Tweets or Weather
checkboxGroupInput("chart", "Select Graphs", choices = c("Tweets", "Weather"), selected = "Tweets")
),
# Plot it!
mainPanel(
tabsetPanel(
# Plot panel
tabPanel("Plot", plotlyOutput('fooPlot1', height = "600px", width = "800px")),
# Insights panel
tabPanel("Insights", textOutput('insights')),
# About panel
tabPanel("About", textOutput('about'))
)
)
)
)
)
library(rtweet)
library(jsonlite)
library(rgeos)
library(rgdal)
library(httr)
library(dplyr)
library(anytime)
# Test Variables (delete before launch!)
# --------------------------------------
start_date <- "2017-05-28"
end_date <- "2017-05-29"
city <- "Portland"
state <- "ME"
day <- "2017-05-28"
# Latitude & Longitude Retrieval for API Calls
# --------------------------------------------
# Code for findLatLong and findGeoData sourced from:
# https://stackoverflow.com/posts/27868207/revisions
# Returns a data frame that contains the longitude and latitude
# for the given state and city.
# Input format: findLatLong(geog_db, "Portland", "ME")
# Ex: lon       lat       city      state
#     -70.25404 43.66186  Portland   ME
findLatLong <- function(geo_db, city, state) {
do.call(rbind.data.frame, mapply(function(x, y) {
geo_db %>% filter(city==x, state==y)
}, city, state, SIMPLIFY=FALSE))
}
# Global Variables (Mara)
# ----------------
# Options list for states and capital cities
cities <- c("Montgomery, Alabama", "Juneau, Alaska", "Phoenix, Arizona",
"Little Rock, Arkansas", "Sacramento, California", "Denver, Colorado",
"Hartford, Connecticut", "Dover, Delaware", "Tallahassee, Florida",
"Atlanta, Georgia", "Honolulu, Hawaii", "Boise, Idaho", "Springfield, Illinois",
"Indianapolis, Indiana", "Des Moines, Iowa", "Topeka, Kansas", "Frankfort, Kentucky",
"Baton Rouge, Louisiana", "Augusta, Maine", "Annapolis, Maryland", "Boston, Massachusetts",
"Lansing, Michigan", "St. Paul, Minnesota", "Jackson, Mississippi", "Jefferson City, Missouri",
"Helena, Montana", "Lincoln, Nebraska", "Carson City, Nevada", "Concord, New Hampshire",
"Trenton, New Jersey", "Santa Fe, New Mexico", "Albany, New York", "Raleigh, North Carolina",
"Bismarck, North Dakota", "Columbus, Ohio", "Oklahoma City, Oklahoma", "Salem, Oregon",
"Harrisburg, Pennsylvania", "Providence, Rhode Island", "Columbia, South Carolina",
"Pierre, South Dakota", "Nashville, Tennessee", "Austin, Texas", "Salt Lake City, Utah",
"Montpelier, Vermont", "Richmond, Virginia", "Olympia, Washington", "Charleston, West Virginia",
"Madison, Wisconsin", "Cheyenne, Wyoming"
)
# Setting minimum start date
min.start <- Sys.Date()-6
# Setting maximum start date
max.start <- Sys.Date()-1
# Setting minimum end date
min.end <- Sys.Date()-5
# Setting maximum end date
max.end <- Sys.Date()
# API Calls - Data Retrieval
# -------------------------
# Retrieves a data frame with the most recent 10000 tweets for a given state and city that were
# tweeted between the given start date and end date.
# Ex: hourly.range          Freq
#     2017-05-28 14:00:00   141
twitterData <- function(city, state, start_date, end_date) {
# Retrieves latitude and longitude for the given state and city for API query
lat.long.df <- geo_data %>% findLatLong(city, state)
longitude <- lat.long.df[,1]
latitude <- lat.long.df[,2]
# Gets 10000 tweets and other information from specified location from the given time range.
twitter.df <- search_tweets(q = " ", geocode = paste0(latitude, ",", longitude, ",","20mi"), n = 10000,
since = start_date, until = end_date, usr = "false")
# Filters dataset to only the column containing the time stamps.
twitter.df.times <- twitter.df %>% select(created_at)
# Generates an hourly range (all of the hours that the tweets occur in) to sort the data by
hourly.range <- cut(twitter.df$created_at, breaks="hour")
# Creates data frame with the number of tweets (Freq) that occur in each hour.
twitter.result <- data.frame(table(hourly.range))
return (twitter.result)
}
# Retrieves a data frame with weather data for the specified day with the given city and state,
# with hourly time block starting from midnight of the day requested,
# continuing until midnight of the following day. Hourly time blocks start from the current system time.
# input format: weatherData("Portland", "ME", "28 May 2017"), multiple Date formats should work
# Ex: temperature     time
#     45.3690         2017-05-27 14:00:00
weatherData <- function(city, state, day) {
# Retrieve latitude and longitude for given city and state
lat.long.df <- geo_data %>% findLatLong(city, state)
longitude <- lat.long.df[,1]
latitude <- lat.long.df[,2]
# Convert given Date to UNIX format
unix.time.day <- as.numeric(as.POSIXct(anydate(day)))
# Retrieve API key from key.JSON (stored in JSON for security)
key <- fromJSON(txt = "access-keys.json")$weather$key
# setting params for API  call
base.url <- "https://api.darksky.net/forecast/"
weather.uri <- paste0(base.url, key, "/", longitude, ",", latitude, ",", unix.time.day)
weather.params <- list(exclude = paste0("currently", ",", "minutely", ",", "flags"))
# retrieving data from API
weather.response <- GET(weather.uri, query = weather.params)
weather.body <- content(weather.response, "text")
weather.results <- fromJSON(weather.body)
# Gets data sorted by hour
weather.df <- weather.results$hourly$data
# convert UNIX time to Dates
weather.df$time <- anytime(weather.df$time)
# convert Celsius temperatures to Fahrenheit
weather.df$temperature <- weather.df$temperature * (9/5) + 32
weather.df <- weather.df %>% select(temperature, time)
return(weather.df)
}
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
# The plot1 variable determines the y axis, therefore, choose the plot that
# has a higher y max
RenderPlots <- function(plot.1, data.1, y.var.1, plot.2, data.2, y.var.2) {
plot.3 <- plot_ly(data = data,
x = data.1[[y.var.1]],
y = data.2[[y.var.2]],
type = "scatter",
marker = list(size = 20,
color = data[[color.var]],
line = list(color = 'rgba(0, 0, 0, .8)',
width = 2),
opacity = 0.7))
return(subplot(plot.1, plot.2, plot.3, shareX = TRUE))
}
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
BuildBarPlot <- function(data, x.var, y.var, x.label, y.label, title, color.var) {
p <- plot_ly(data = data,
x = data[[x.var]],
y = data[[y.var]],
type = "bar",
color = color.var #fixxxx
) %>%
layout(
title = title,
xaxis = list(title = x.label),
yaxis = list(title = y.label),
barmode = "group"
)
p <- hide_colorbar(p)
# layout(
#   yaxis = list(range = c(0, 100))
# )
return(p)
}
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
BuildLinePlot <- function(data, x.var, y.var, x.label, y.label, title, color.var){
p <- plot_ly(data = data,
x = data[[x.var]],
y = data[[y.var]],
type = "scatter",
marker = list(size = 20,
color = data[[color.var]],
line = list(color = 'rgba(0, 0, 0, .8)',
width = 2),
opacity = 0.7)) %>%
layout(title = title,
yaxis = list(title = x.label),
xaxis = list(title = y.label))
return(p)
}
runApp()
runApp()
runApp()
runApp()
traceback()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages('rjson')
library(anytime)
library(shiny)
library(dplyr)
library(plotly)
library(httr)
library(rgeos)
library(jsonlite)
library(rgdal)
library(rtweet)
library(stringr)
library(rjson)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(rtweet)
library(jsonlite)
library(rgeos)
library(rgdal)
library(httr)
library(dplyr)
library(anytime)
# Test Variables (delete before launch!)
# --------------------------------------
start_date <- "2017-05-28"
end_date <- "2017-05-29"
city <- "Portland"
state <- "ME"
day <- "2017-05-28"
# Latitude & Longitude Retrieval for API Calls
# --------------------------------------------
# Code for findLatLong and findGeoData sourced from:
# https://stackoverflow.com/posts/27868207/revisions
# Returns a data frame that contains the longitude and latitude
# for the given state and city.
# Input format: findLatLong(geog_db, "Portland", "ME")
# Ex: lon       lat       city      state
#     -70.25404 43.66186  Portland   ME
findLatLong <- function(geo_db, city, state) {
do.call(rbind.data.frame, mapply(function(x, y) {
geo_db %>% filter(city==x, state==y)
}, city, state, SIMPLIFY=FALSE))
}
# Global Variables (Mara)
# ----------------
# Options list for states and capital cities
cities <- c("Montgomery, Alabama", "Juneau, Alaska", "Phoenix, Arizona",
"Little Rock, Arkansas", "Sacramento, California", "Denver, Colorado",
"Hartford, Connecticut", "Dover, Delaware", "Tallahassee, Florida",
"Atlanta, Georgia", "Honolulu, Hawaii", "Boise, Idaho", "Springfield, Illinois",
"Indianapolis, Indiana", "Des Moines, Iowa", "Topeka, Kansas", "Frankfort, Kentucky",
"Baton Rouge, Louisiana", "Augusta, Maine", "Annapolis, Maryland", "Boston, Massachusetts",
"Lansing, Michigan", "St. Paul, Minnesota", "Jackson, Mississippi", "Jefferson City, Missouri",
"Helena, Montana", "Lincoln, Nebraska", "Carson City, Nevada", "Concord, New Hampshire",
"Trenton, New Jersey", "Santa Fe, New Mexico", "Albany, New York", "Raleigh, North Carolina",
"Bismarck, North Dakota", "Columbus, Ohio", "Oklahoma City, Oklahoma", "Salem, Oregon",
"Harrisburg, Pennsylvania", "Providence, Rhode Island", "Columbia, South Carolina",
"Pierre, South Dakota", "Nashville, Tennessee", "Austin, Texas", "Salt Lake City, Utah",
"Montpelier, Vermont", "Richmond, Virginia", "Olympia, Washington", "Charleston, West Virginia",
"Madison, Wisconsin", "Cheyenne, Wyoming"
)
# Setting minimum start date
min.start <- Sys.Date()-6
# Setting maximum start date
max.start <- Sys.Date()-1
# Setting minimum end date
min.end <- Sys.Date()-5
# Setting maximum end date
max.end <- Sys.Date()
# API Calls - Data Retrieval
# -------------------------
# Retrieves a data frame with the most recent 10000 tweets for a given state and city that were
# tweeted between the given start date and end date.
# Ex: hourly.range          Freq
#     2017-05-28 14:00:00   141
twitterData <- function(city, state, start_date, end_date) {
# Retrieves latitude and longitude for the given state and city for API query
lat.long.df <- geo_data %>% findLatLong(city, state)
longitude <- lat.long.df[,1]
latitude <- lat.long.df[,2]
# Gets 10000 tweets and other information from specified location from the given time range.
twitter.df <- search_tweets(q = " ", geocode = paste0(latitude, ",", longitude, ",","20mi"), n = 10000,
since = start_date, until = end_date, usr = "false")
# Filters dataset to only the column containing the time stamps.
twitter.df.times <- twitter.df %>% select(created_at)
# Generates an hourly range (all of the hours that the tweets occur in) to sort the data by
hourly.range <- cut(twitter.df$created_at, breaks="hour")
# Creates data frame with the number of tweets (Freq) that occur in each hour.
twitter.result <- data.frame(table(hourly.range))
return (twitter.result)
}
# Retrieves a data frame with weather data for the specified day with the given city and state,
# with hourly time block starting from midnight of the day requested,
# continuing until midnight of the following day. Hourly time blocks start from the current system time.
# input format: weatherData("Portland", "ME", "28 May 2017"), multiple Date formats should work
# Ex: temperature     time
#     45.3690         2017-05-27 14:00:00
weatherData <- function(city, state, day) {
# Retrieve latitude and longitude for given city and state
lat.long.df <- geo_data %>% findLatLong(city, state)
longitude <- lat.long.df[,1]
latitude <- lat.long.df[,2]
# Convert given Date to UNIX format
unix.time.day <- as.numeric(as.POSIXct(anydate(day)))
# Retrieve API key from key.JSON (stored in JSON for security)
key <- fromJSON(file = "access-keys.json")$weather$key
# setting params for API  call
base.url <- "https://api.darksky.net/forecast/"
weather.uri <- paste0(base.url, key, "/", longitude, ",", latitude, ",", unix.time.day)
weather.params <- list(exclude = paste0("currently", ",", "minutely", ",", "flags"))
# retrieving data from API
weather.response <- GET(weather.uri, query = weather.params)
weather.body <- content(weather.response, "text")
weather.results <- fromJSON(weather.body)
# Gets data sorted by hour
weather.df <- weather.results$hourly$data
# convert UNIX time to Dates
weather.df$time <- anytime(weather.df$time)
# convert Celsius temperatures to Fahrenheit
weather.df$temperature <- weather.df$temperature * (9/5) + 32
weather.df <- weather.df %>% select(temperature, time)
return(weather.df)
}
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(mtcars)
runApp()
runApp()
runApp()
runApp()
runApp()
# buildtimeline.R (ESHA)
# For tweets: bar chart of tweets over time (answers: when do people tweet the most?)
# For weather: line graph of temperature (scale so it will sit on same graph as tweets)
# If both are checked, render them on top of one another
# Add correlation
library(plotly)
library(ggplot2)
library(dplyr)
BuildLinePlot <- function(data, x.var, y.var, x.label, y.label, title, color.var){
p <- plot_ly(data = data,
x = data[[x.var]],
y = data[[y.var]],
type = "scatter",
marker = list(size = 20,
color = data[[color.var]],
line = list(color = 'rgba(0, 0, 0, .8)',
width = 2),
opacity = 0.7)) %>%
layout(title = title,
yaxis = list(title = x.label),
xaxis = list(title = y.label))
return(p)
}
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
